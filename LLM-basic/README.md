# Basic LLM Training on ArXiv Data

Welcome to the **LLM-basic** directory! This repository contains scripts and resources for training a basic Language Model (LLM) on an ArXiv dataset. The purpose of this project is to collect data from ArXiv, explore the dataset, process the collected data, and train a basic Language Model, while exploring some methodology involved with improving training and prediction performance.

## Getting Started

1. **Clone the Repository**: Clone this repository to your local machine using `git clone`.

2. **Install Dependencies**: Ensure you have all the necessary dependencies installed. I recommend setting up a venv like so:
```
python -m venv LLM_env
source LLM_env/bin/activate
pip install -r requirements.txt
```

3. **Data Collection**: Run `data_collection.py` to collect data from the open ArXiv API. You may need to run this script multiple times with different topics in order to get a variety of training data. Alternatively, you can download a dataset I've already collected from [here](https://drive.google.com/file/d/1RxVmHi96jF1UpUdG7HCsX6ML7B45QLKS/view?usp=sharing).

4. **Explore the Data**: Execute `explore_data.py` to get insights into the collected dataset. This will help you understand its size and characteristics.

5. **Process the Dataset**: Run `process_dataset.py` to preprocess the dataset. This script will tokenize the `.tex` files in `dataset_raw` and save the tokenized results and tokenizer under `dataset_processed` for later use in training the Language Model.

6. **Train the Language Model**: With the dataset processed, you can now proceed to train your Language Model using the tokenized data, with `guild run train`.

## Scripts

### 1. `data_collection.py`
- **Description**: This script collects data from the ArXiv API endpoint.
- **Usage**: Run this script to collect data from ArXiv and save it for further processing.

### 2. `explore_data.py`
- **Description**: This script provides a quick look at the dataset collected, including generating a histogram of file sizes.
- **Usage**: Execute this script to explore the characteristics of the dataset collected from ArXiv.

### 3. `process_dataset.py`
- **Description**: This script processes the collected data. It pulls in all `.tex` files collected, fits a tokenizer to them, and saves the tokenizer and the tokenized files.
- **Usage**: Run this script to preprocess the dataset, tokenize the text files, and save the tokenizer for later use in training the Language Model.

### 4. `get_data_subset.py`
- **Description**: This script grabs a random subset of training sequences and writes them to a file. I've used this for selecting data on which I'll evaluate the human baseline.
- **Usage**: After running this script, see result inputs in `out_x.txt` and targets in `out_y.txt`.

### 5. `train_model.py`
- **Description**: This script creates and trains the language model on the token sequences generated by `process_dataset.py`.
- **Usage**: Invoking this script with `guild run train` will save the results in a format viewable by GuildAI. To inspect the output, run `guild view`.



Happy training! ðŸš€